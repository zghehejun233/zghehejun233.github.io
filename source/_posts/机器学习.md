---
title: 机器学习
description: 机器学习吧啦吧啦
categories: AI
tags:
  - ML
  - DL
  - SL
  - SSL
  - RL
index_img: /post_img/91335433_p0_master1200.jpg
banner_img: /post_img/91335433_p0_master1200.jpg
abbrlink: 29139
header: ''
---
## Introduction

### Different Types

- Regression: outputs a scalar.
- Classification: outputs one of the **given options**.
- Structed Learning: create something with structure(e.g.document, image).

通常来说, 机器学习的方法包括[^6]

- Supervised Learning
- Unsupervised Learning
- Semi-supervised Learning
- Reinforcement Learning
- Genetic Algorithm, 遗传算法

### Steps to Train a Model

Training:

1. Function with unknown parameters(based on domain knowledge, to guess a model with some parameters), also called **Model**
2. Define loss(a function inside of the model) from training data to describe how good a set of value is.
3. Optimization(e.g. Gradient Descent).

> 习惯上，从数学的角度，我们取weight为w表示具有权重内涵的参数，bias为b表示具有偏移内涵的参数。

Loss function $L$ such as

- Mean Absolute Error(MAE), &nbsp;$e=|y-\hat{y}|$
- Mean Square Error(MSE), &nbsp;$e=(y-\hat{y})^2$

将$L$绘制成图，称为Erroe Surface.

训练时自行指定的参数称为hyperparameter.

### Sigmoid Function

线性方程能拟合的非常有限，这个问题有一个专有的称呼，Model Bias。

**All Piecewise Linear Curve = Constant + Sum of a Set of Polylines.**

Sigmoid Function可以构造各种曲线或模拟折线

$$
y=c{\frac{1}{1+e^{-(b+w{x_1})} } }=c{\cdot}sigmoid(b+w{x_1})
$$

- w: 改变形状，或者说倾斜程度
- b: 改变水平偏移
- c: 改变高度范围

讲自变量扩展到$j$个，这样任何一个曲线可以表示为

$$
y={b_0}+\sum_i{ {c_i}{\cdot}sigmoid({b_i}+{\sum_j{w_{ij}{x_j} } })}
$$

$\mathbf{x}$ is called **features**; others are called $\mathbf{\theta}$.

通过引入sigmoid函数可以更好的设计第一步的方程(function with unknown)。

### Optimization of New Model

目标是通过Loss function $L(\mathbf{\theta})$ 找到一个$\mathbf{\theta^*}={\mathit{arg}{\underset{\mathbf{\theta} }{\mathrm{min} } }L}$

数据集可以分为若干个batch，使用某个batch也可以进行一次update，这样逐一通过每个batch进行一次运算的过程称为epoch。

## 神经网络简介

### Neural Network

### Convolutional Neural Network, CNN

### Recurrent Neural Network, RNN

#### Long-short Term Memory, LSTM

### Autoencoder

### Generative Adversarial Nets

### Transfer Learning

## Introduction to Deep Learning

### History and Models

- 1958: Perceptron
- 1980: Multi-layer Percrptron，与现在的DNN没啥区别
- 1986: Backpropagation
- 2006: RBM initialization
- 2009: GPU
- 2011: 引入语音电视

所谓的Deep Learning就是指有很多隐藏层的神经网络模型。

### Backpropagation

$$
\frac{\partial{C} }{\partial{\mathbf{w} } }={\frac{\partial{\mathbf{z} } }{\partial{\mathbf{w} } } }\cdot {\frac{\partial{C} }{\partial{\mathbf{z} } } }
$$

前半部分就被叫做forward pass，每一个$w$对$z$的偏微分值与他的输入相等。

另一个部分被称为backward pass

$$
\begin{array}{l}
  {\frac{\partial{C} }{\partial{z} } }&={\frac{\partial{a} }{\partial{z} } }{\frac{\partial{C} }{\partial{a} } }\\
  {\frac{\partial{a} }{\partial{z} } }&={\sigma}'(z)\\
  {\frac{\partial{C} }{\partial{a} } }&=\sum_i{\frac{\partial{z^{(n)} } }{\partial{a} } }{\frac{\partial{C} }{\partial{z^{(n)} } } }\\
  &=...
\end{array}
$$

### Overfitting

我们使用的输入数据显然是不完备的，基于不完备信息得出的pattern也自然是不准确的，这种不准确表达出来就是两种，一种是欠拟合, underfitting，另一种是过拟合, overfitting[^3]。

- Underfitting通常是针对训练集，例如我们的预测pattern过于平滑（e.g 直线来拟合多个输入点），也就是模型过于简单，这个通常可以通过引入多项式，或者神经网络等复杂模型来解决。
- Overfitting则通常是说训练习得的model对于测试集性能很差，而对训练集性能很好，也就是对训练集是overfitting，有很差的Generalization（泛化能力）。

### Regularization

$$
L=\sum{({\hat{y}^n}-(b+\sum{ {w_i}{x_i} }))^2}\;+\;\lambda\sum{(w_i)^2}
$$

目的是为了得到更加平滑的模型。

正规化（regularization)：也就是在loss function中增加一个regularization term来平衡generalization和overfitting，$\lambda$是正则变量，也是一个hyperparameter，是我们自己来设定的[^3]。

有了这个term，我们可以发现，$\lambda$越大，这个term的权重也越大，那么相应的w参数权重越小，也就是曲线越平滑，极端情况，$\lambda$很大，那么w的各个参数趋近于0，从而使得pattern接近于一条直线（underfitting）；$\lambda$越小，整个term的权重也越小，相应的w的权重越大，曲线则越不平滑，极端情况$\lambda=0$则退化为没有正规化的loss function，从而overfitting。

> lambda这样的hyperparameter的选取通常是得不到数学的保障的，所以为什么很多人将ML的model选取和古代道士炼丹一样（更多的是经验和运气）。当然我们可以plot不同的lambda的选择形成的loss function，从而选择合适的lambda（似乎比道士高不了多少）。

### Classification

Loss function

$$
L(f)=\sum_n{\delta(f(x^n)\neq{\hat{y} }^n)}
$$

Generative model

$$
P(x)=P(x|C_1)P(C_1)+P(x|C_2)P(C_2)+{\dotsb}
$$

分类任务可以使用一个Gaussian Distribution $L(\mu,\sum)$模拟。可以得到

$$
{\mu^*}={\frac{1}{79} }(\sum_{n=1}^{79}{x^n}),\;
{\sum^*}={\frac{1}{79} }{\sum_{n=1}^{79} }({x^n}-{\mu^*})({x^n}-{\mu^*})^T
$$

通常对于不同的分类，会选择给所有分类同一个covarience matrix减少参数数量，避免过拟合。

### Logistic Regression

![image-20220710204333711](https://tva1.sinaimg.cn/large/e6c9d24ely1h4248fflsnj213q0tz78i.jpg)

一般Linear Regression是解决回归问题，而 Logistic Regression是应用于分类问题。Logistic Regression除了可以解决二分类问题外，还可以解决多分类问题[^5]。

采用 Logistic Regression进行多分类的思路是：选取某个分类作为正样本，其他分类作为负样本建立一个二分类模型；以此类推建立多个（有几个分类就建几个）二分类模型；对多个二分类模型的输出值进行大小比较，把样例归为输出值最大的那类[^4]。

## Convolutional Neural Network

## 模型拟合失败

### Local Minima and Saddle Point

Local minima和saddle point被称为critical point。

#### Tayler Series Approximation

$$
{L(\theta)}\; \approx \;{L(\theta^*)\,+\,{ {(\theta-\theta^*)^T}g}\,+\,{ {\frac{1}{2} }{(\theta-\theta^*)^T}H{(\theta-\theta^*)} } }
$$

实际上，由于训练的模型维度往往非常高，所以并不会经常陷入局部最优的陷阱。

要说起来，卡在Saddle Point的可能性还是有的。

### Batch and Momentum

#### Batch

一般称为批次。每次运算后会刷新一次batch，这个过程被称为 shuffle。

合适的batch可以减少每次计算梯度的运算量，并且其期望与计算所有梯度相等，在大训练数据的情况下是十分有效的一种方式。

![image-20220711204707803](https://tva1.sinaimg.cn/large/e6c9d24ely1h439yhzqz5j21p60owgnx.jpg)

由于GPU具有强大的并行运算能力，所以从左图可以发现一定数量的batch并不会带来运算时间的显著增长；反而是过小的batch会导致更多的update，而update是同步进行的，所以在总时间消耗上反而会更长。

![image-20220711204953301](https://tva1.sinaimg.cn/large/e6c9d24ely1h43a1be0drj21sa0tmq5y.jpg)

这个比较反映出大batch有更低的精确度，这或许与陷入局部最优有关。较大的batch相当于添加了一定噪声，这些噪声是有效果带领模型摆脱陷阱的。

> 这种噪声类似进化模型里的随机变量。

#### Momentum

Momentum，动量，算是对梯度下降的改进。

在计算每次参数变化时，引入了上一次变化值作为参数

$$
m^t={\lambda m^{t-1} }-{\eta {g^{t-1} } }
$$

### Auto Learning Rate

![image-20220711210148269](https://tva1.sinaimg.cn/large/e6c9d24ely1h43adoyljhj20fo0fcdfx.jpg)

有时候。。会陷入这种摇摆的情况。显然，在Loss越小的情况下，Learning rate就应该越小，不然可能步子迈大了扯着蛋。

可以使用这种形式改进

$$
{\theta^{t+1}_{i} }={\theta^{t}_{i} }-{\frac{\eta}{\sigma^{t}_{i} }{g^{t}_{i} } }
$$

#### Root Mean Square

$$
\begin{array}{l}
  {\sigma^{0}_{i} }={\sqrt{({g^{0}_{i})}^2} }\\
  {\sigma^{1}_{i} }={\sqrt{ {\frac{1}{2} }[({g^{0}_{i})^2+({g^{1}_{i} })^2]} } }\\
  {\sigma^{n}_{i} }={\sqrt{ {\frac{1}{n+1} }{\sum_{j=0}^n{(g^{j}_{i})^2 } } } }
\end{array}
$$

#### RMSProp

$$
{\sigma^{n}_i}=\sqrt{ {\alpha{(\sigma^{n-1}_i)^2} }+{(1-\alpha){(g^{n}_i)^2} } }
$$

#### Learning Rate Scheduling

$$
{\theta^{t+1}_{i} }={\theta^{t}_{i} }-{\frac{\eta^t}{\sigma^{t}_{i} }{g^{t}_{i} } }
$$

- Learning rate decay: 随次数增加 Learning rate 减小
- Warm Up: Learning rate increase and then decrease.

### Influence of Loss Function

主要影响分类问题

#### One-hot Vector

使用 One-hot vector 来表示分类，而不是 `1, 2` 这样的序号，可以排除数字大小的影响。

此时模型的结构需要加入softmax函数

![image-20220711213524697](https://tva1.sinaimg.cn/large/e6c9d24ely1h43bco6arnj21j00m8q52.jpg)

此时的 Loss Function，由于 output 是一对 one-hot vector，使用 MSE 计算就显得不太好了。应该使用 cross entropy 计算。

> Minimizing cross entropy is equivalent to maxmizing likelihood.

### Summary of Optimization

最后的update模型为

$$
{\theta^{t+1}_i}\; =\; {\theta^t_i} - { {\frac{\eta^t}{\sigma^t_i} } {m^t_i} }
$$

- $\eta^t$: Learning rate scheduling.
- $\sigma^t_i$: Root mean square of the gradients.
- $m^t_i$: Weighted sum of the previous gradients.

## References

[^1]: [李宏毅2022机器学习深度学习课程](https://www.bilibili.com/video/BV1JK4y1D7Wb?p=2&share_source=copy_web)

[^2]: [机器学习之过拟合(overfitting) - 朱涛的文章 - 知乎](https://zhuanlan.zhihu.com/p/25720278)

[^3]: [什么是 L1/L2 正则化 (Regularization) - 莫烦的文章 - 知乎](https://zhuanlan.zhihu.com/p/25707761)

[^4]: [Linear Regression 与 Logistic Regression的几点不同 - jingyi130705008 - CSDN](http://t.csdn.cn/y8ANf)

[^5]: [[机器学习笔记] Logistic Regression与Linear Regression的区别 - 梅森上校 - CSDN](http://t.csdn.cn/rHAnh)

[^6]: [多种多样的机器学习 - 莫烦 - 莫烦Python](https://mofanpy.com/tutorials/machine-learning/ML-intro/machine-learning/)
