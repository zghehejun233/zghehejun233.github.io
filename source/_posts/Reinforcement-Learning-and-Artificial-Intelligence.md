---
title: Reinforcement Learning and Artificial Intelligence
description: æš‘æœŸè¯¾ç¨‹å¼ºåŒ–å­¦ä¹ ä¸äººå·¥æ™ºèƒ½ç¬”è®°
categories: AI
tags:
  - RL
abfrlink: 48478
index_img: /post_img/81514130_p0_master1200.jpg
banner_img: /post_img/81514130_p0_master1200.jpg
abbrlink: 48478
header: ''
---
## Introduction

Reinforcement Learning (RL) is a **scientific theory** (or hypothesis) that seeks to provide a **computational explanation** to the widely **observed phenomenon** that some complex **physical system** can adapt to the **physical environment** around it.

- scientific theory: links an observation (i.e. the subject) to some other reproducible observations (i.e. the evidence) with logic.
- computational explanation
- observed phenomenon
- physical system
- physical environment

The Rl regard the physical world as a Markov chain. Consider a physical system that interacts with a surrounding environment by exchanging mass/energy across the system boundary. The **world (system + environment)** changes over time, and is in a certain state $\mathbf{s_t}$ at each moment $t$.

Different physical theories may formulate $\mathbf{s_t}$ into different mathematical objects ($s$ can be a vector, a tensor, a function, a function of functions, etc.). But in most physical theories, the time evolution of the state follows time-invariant
(possibly stochastic) transition laws, such that $\mathbf{s_t}$ decouples the past from the future.

> Theorem (H. M. Sheffer, in Transactions of the American Mathematical Society, 1913): Every digital computer is functionally equivalent to an NAND network.

## Math Basics

### Probability Theory Basics

> è¯¦è§æ¦‚ç‡è®ºç›¸å…³çš„æ–‡ç« ï¼Œè¿™é‡Œæ‘˜å½•ä¸€ä¸‹è¯¾ä¸Šçš„å¤§çº²

#### Probability Space

- Sample Space, a set of all â€œpossible worldsâ€ under concern, each with full certainty and definiteness.
- Event Space, a set of all random events; each event may occur in a subset of the possible worlds.
- Probability function, mapping that assigns a real number to each random event to indicate its â€œpropensity of occurrenceâ€.

#### Joint Probability, Union Probability, Union Bound

Union Bound:

$$
\begin{array}{c}
  P[{A_1}or{A_1}or{\dotsb}{ {A_n} }]{\le}\sum_{i=1}^{n}P[A_i]
\end{array}
$$

#### Conditional Probability, Independence

The conditional probability has been in a new probability space, so much different with joint probability.And to point it, use $P_B[A]$ is better than $P[A|B]$.

To change the probability space

$$
\begin{array}{c}
  P[AB]=P[A|B]P[B]=P[B|A]P[A]
\end{array}
$$

And if we want to add a probability of C

$$
\begin{array}{c}
  P[AB|C]=P[A|BC]P[BC], or this form\\
  P_C[AB]=P_C[AB]P_C[B].\\
\end{array}
$$

#### Random Variable

If $X$ is continuous, we will (ab-)use ğ[ğ‘‹ = ğ‘¥] to refer (instead) to the probability density of $X = x$.

If ğ‘‹ is continuous, we will (ab-)use Î£ğ‘¥ (instead of âˆ« dğ‘¥) to refer to the probabilistic integration

#### Probability Distribution

A function ğ‘· is a probability distribution function if and only if (1) ğ‘ƒ ğ’™ â‰¥ ğŸ and (2) Ïƒğ’™ ğ‘ƒ(ğ’™) = ï¿½

#### Multivariate Variable, Categorical Variable

- Multivariate variable (random vector), a vector of random variables.
- Categorical variable, rand. var. whose values are â€œlabelsâ€ (rather than â€œnumbersâ€)
- One-hot vectors, numerical representation for categorical variables

#### Function of Random Variable

For an ordinary, definite, and deterministic function, its output becomes a random variable if the input of the function is a random variable.

ğ ğ‘“ ğ‘‹ = ğ‘¦ = Ïƒ{ğ‘¥: ğ‘“ ğ‘¥ =ğ‘¦} ğ[ğ‘‹ = ğ‘¥]

#### Expectation, Conditional Expectation and Function of Random Variable's Expectation

Expectation, also called expected value, or mean value, or simply the mean, is the probability-weighted average over all possible values of a random variable.

conditional expectation is conditioned on a random event, not on a random variable.

For a function, $P[f(X)=y]=\sum_{\{x:f(x)=y\} }P[X=x]$, and $E[f(X)]=\sum_{x}P[X=x]{\cdot}f(x)$

typically used as **distance unit** to measure how far a particular value of ğ‘‹ deviates from the mean ğ„[ğ‘‹]

#### Variance, Standard Deviation

The variance of ğ‘‹ is the probability-weighted average of the squared distances between possible values of ğ‘‹ and its mean

$$
\begin{array}{c}
  Var[X]=E[(X-E[X])^2], and conveniently\\
  Var[X]=E[X^2]=(E[X])^2
\end{array}{c}
$$

Standard deviation $\sigma(X)=\sqrt{Var[X]}$

#### Covariance and Correlation

The covariance between two random variables ğ‘‹ and ğ‘Œ measures the tendency that the values of ğ‘‹ and ğ‘Œ jointly deviate in the same way from their respective mean values.

$$

$$

Pearson correlation coefficient normalizes the deviations by dividing the standard deviations, so that we always have 0 â‰¤ ğœŒ(ğ‘‹, ğ‘Œ) â‰¤ 1

For multivariate random variable ğ‘¿ = ğ‘‹1, ğ‘‹2, â€¦ ,ğ‘‹ğ‘› T, the covariance matrix of ğ‘¿consists of all the pairwise covariances between the elements in ğ‘¿

#### Entropy and Cross Entropy

The entropy of a random variable ğ‘‹ characterizes the inherent uncertainty of ğ‘‹, or equivalently, the â€œexpected surpriseâ€ when observing a sample of ğ‘‹.

The uniform distribution is the maximum-entropy distribution among all probability distributions with the same support.

For two random variables ğ‘‹ and ğ‘Œ with the same support, the cross entropy of ğ’€relative to ğ‘¿ measures **how ğ‘Œâ€™s distribution differs from ğ‘‹â€™s distribution**.

Cross entropy (or more accurately, ğ‘¯ ğ’€ ğ‘¿ âˆ’ ğ‘¯(ğ‘¿), called the KL-divergence) is usually used as a measure of the **distance between two distributions**

### Parametric Models in Statistics

#### Gaussian Model

Gaussian distribution is so widely observed that itâ€™s been called, the normal

å½“æ ·æœ¬æ•°æ® X æ˜¯ä¸€ç»´æ•°æ®ï¼ˆUnivariateï¼‰æ—¶ï¼Œé«˜æ–¯åˆ†å¸ƒéµä»ä¸‹æ–¹æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆProbability Density Functionï¼‰<sup>2</sup>ï¼š

$$
N(x;\mu,\sigma^2)={\frac{1}{\sigma \sqrt{2\pi} }}e^{ {-\frac{1}{2} }(\frac{x-\mu}{\sigma})^2}
$$

> A parametric model that well captures the probability distribution of not a single, but many random variables encountered in the real world.

**Gaussian distribution** has a single â€œpeakâ€ (called the mode of the distribution). For many random variables $Y$ in the real world, its marginal distribution $P(y)$ is **multi-modal**.

Sometimes such $P(y)$ can be decomposed into a mixture of unimodal distributions.

$$
P(y)=\sum_{x}P(x)P(y|x)
$$

where $X$ is another observable random variable.

we can fit the conditional distribution $P(y|x)$ , with a separate Gaussian function, for each $x$

$$
f(y;\mathbf{\theta})=\sum_{x}P(x)N(y;\mu_x,\sigma_x^2), \\
where {\quad}\mathbf{\theta}=({\mu_1},{\mu_2},\dotsb,{\mu_n},{\sigma_1},{\sigma_2},\dotsb,{\sigma_n})
$$

**Gaussian Mixture Model, GMM** method seeks to estimate $\mathbf{\theta}$ for the mixture distribution $f$ without observing $X$.

é«˜æ–¯æ··åˆæ¨¡å‹å¯ä»¥çœ‹ä½œæ˜¯ç”± K ä¸ªå•é«˜æ–¯æ¨¡å‹ç»„åˆè€Œæˆçš„æ¨¡å‹ï¼Œè¿™ K ä¸ªå­æ¨¡å‹æ˜¯æ··åˆæ¨¡å‹çš„éšå˜é‡ï¼ˆHidden variableï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ªæ··åˆæ¨¡å‹å¯ä»¥ä½¿ç”¨ä»»ä½•æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™é‡Œä½¿ç”¨é«˜æ–¯æ··åˆæ¨¡å‹æ˜¯å› ä¸ºé«˜æ–¯åˆ†å¸ƒå…·å¤‡å¾ˆå¥½çš„æ•°å­¦æ€§è´¨ä»¥åŠè‰¯å¥½çš„è®¡ç®—æ€§èƒ½<sup>2</sup>ã€‚

We can further factorize $X$ into a mixture distribution, depending on some $Z$, and so on, eventually leading to a **Probabilistic Graphical Model** (called **Bayesian Network**)

#### Gaussian Linear Model[^1]

When $X$ has infinite support, we canâ€™t parameterize $f$ with a finite number of $\mu x$ and $\sigma x$.

In this case, we can make $\mu$ (and $\sigma$) a function of $x$, and parameterize $f$ through parameterizing the functions $\mu(x)$ and $\sigma(x)$

Gaussian Linear Models (or Linear Gaussian Models) are parametric models that seek to fit the **conditional probabilities** $P[Y=y|X=x]$, where both $X$ and $Y$ are observablerandom variables, with a Gaussian function with linear mean:

$$
f(y|x;a,b,\sigma)=N(y;ax+b,\sigma)={\frac{1}{\sigma{\sqrt{2\pi} }} }e^{ {-\frac{1}{2} }{(\frac{y-(ax+b)}{\sigma})}^2}
$$

With separately learned distribution $P(x)$, the Gaussian Linear function $f(y|x;a,b,\sigma)$can model more complex marginal distribution of $Y$ (than a single Gaussian function can), $P[Y=y]=\sigma_x{P(x)f(y|x;a,b,\sigma)}$.

Often, we just care about the conditional probability for its own sake (e.g. in policy learning), $P[Y=y|X=x]=f(y|x;a,b,\sigma)$.

#### Maximum Likelihood Estimation, MLE

å¯¹äºå•é«˜æ–¯æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æœ€å¤§ä¼¼ç„¶æ³•ä¼°ç®—å‚æ•° $\mathbf{\theta}$çš„å€¼[^2]

$$
\prod^{n}_{i=1}{\mathbf{P_{\theta} }}[{Y_i}={y_i}|{X_i}={x_i}]=\prod^{n}_{i=1}f({x_i,y_i;\mathbf{\theta} })
$$

The MLE principle proposes to choose the parameter vector $\mathbf{\theta}$ that maximizes the function, which is equivalent to minimize the **negative-log-likelihood (NLL) function**:

$$
-\log{\prod^{n}_{i=1}f({x_i,y_i;\mathbf{\theta} })}=-{\sum_i}\log f({x_i,y_i;a,b,\sigma})={\frac{1}{2\sigma^2} }{({\sum_i}(a{x_i}+b-{y_i})^2)}+n\log{\sigma{\sqrt{2\pi} }}
$$

#### Gaussian Linear Model with Multivariate

Multivariate Gaussian Linear Model is a parametric model that seeks to fit the conditional distribution $Y=y|\mathbf{X}=\mathbf{x}$, where $\mathbf{X}$ is an observable **multivariate random variable**, such that

$$
f(y|\mathbf{x};\mathbf{w},\sigma)=N(y;{\mathbf{w}\cdot\mathbf{x} },\sigma)={\frac{1}{\sigma{\sqrt{2\pi} }} }e^{ {-\frac{1}{2} }{(\frac{y-{\mathbf{w}\cdot\mathbf{x} }}{\sigma})}^2}
$$

#### Gasussian Linear Regression and Mean Squared Error, MSE

The Gaussian linear function can be alternatively viewed as a parametric model for a â€œperturbed mappingâ€ from a vector variable ğ’™ to a scalar variable ğ‘¦, where the perturbation is a Gaussian noise $\epsilon ~ N(0,\sigma^2)$

Given observations and the parameter vector as the linear coefficients, can be equivalently chosen based on the **Mean Squared Error (MSE) principle**, without the explicit probabilistic interpretation

$$
{L_{MSE} }={\frac{1}{n} }\sum_i{(\mathbf{w\cdot x}-{y_i})^2}
$$

#### Logistic Regression

æ·±åº¦å­¦ä¹ çš„å®è·µè¯´æ˜ï¼Œçº¿æ€§å›å½’æ€»æ˜¯å­˜åœ¨ä»–çš„å±€é™æ€§ã€‚

Generalized linear model in the form of

$$
g(\mathbb{}{x};\mathbb{w})=\phi(\mathbf{w}\cdot \mathbf{x}), {\quad}\phi(s)=\frac{1}{1+e^{-s} }
$$

or other function with the similar S-shape curve

#### Deep Logistic Networks

## Elements in Reinforcement Learning

**RL is a Cross-disciplinary nature.**

![image-20220705010806726](https://tva1.sinaimg.cn/large/e6c9d24ely1h3ve5vqbuoj20xa0u0q5v.jpg)

### Origin of RL

Reinforcement: The observation that under some conditions, some stimulus has the effect that it can increase the likelihood of the action preceding it, with a reproducible pattern.

Behavioral psychology research not only gave birth to the name of RL, but more importantly, provides abundant experimental data for the RL theory to explain.

### The Policy

Policy is changing during learning.

When a system learns, the mapping rule between its inputs (=observations) and outputs (=actions) must be changing.

![image-20220705011701586](https://tva1.sinaimg.cn/large/e6c9d24ely1h3vef3efh3j20na0b03yy.jpg)

The mapping rule is called **a decision policy function**, or just **policy** for short.

- Deterministic policy: $\mathbf{a_t}=\pi(\mathbf{x_t})$
- Stochastic policy: $P[{A_t}=\mathbf{a_t}|{X_t}=\mathbf{x_t}]=\pi(\mathbf{x_t},\mathbf{a_t})$,so that ${A_t}~\pi({x_t},\cdot)$

#### Parameterization

For any system to learn anything, its policy $\pi$ must be able to change flexibly

With a parameter $\theta$, we get

- Deterministic policy: $\mathbf{a_t}=\pi(\mathbf{x_t},\theta)$
- Stochastic policy: $P[{A_t}=\mathbf{a_t}|{X_t}=\mathbf{x_t}]=\pi(\mathbf{x_t},\mathbf{a_t},\theta)$

To change the model $\pi$, we only need to change its parameter $\theta$.

For flexible learning, we want the model to cover a large enough set of possible policy functions. For example, a linear model is impossible to learn effectively if the optimal policy is $a=x^2$.

#### Trial and Error

Trial and Error is family of optimization methods that can optimize an objective function $J$ without explicit knowledge (e.g. the closed form) about $J$.

The trial-and-error process results in a trace of policies with improving performance

### Related models(preview)

About parameterization:

- Regression models in statistics
- Parameterization in biological neuronal networks
- Artificial neural networks
- From Genotype to Biological Fitness

About trial and error:

- Evolution as a special form of trial-and-error learning
- Evolutionary algorithms in AI
- Policy Gradient Method, Stochastic Gradient Descent (SGD)
- Bandit Algorithms, Upper Confidence Bound (UCB) Algorithm
- Q-Learning
- Monte-Carlo Tree Search
- Correspondence in Cognitive Neuroscience
- Deep Reinforcement Learning

## Biological Neural Nwtwork

Brain consists of two types of cells: glial cells and neurons.

![image-20220706152706244](https://tva1.sinaimg.cn/large/e6c9d24ely1h3x8m229o2j218e0u0gr7.jpg)

![image-20220706152905711](https://tva1.sinaimg.cn/large/e6c9d24ely1h3x8nz6x74j21aa0fktbd.jpg)

### Integrate-and-fire Model

**Action potential** in pre-synaptic neuron induces **response current** in post-synaptic neuron.Resoonse current decays over time.

$$
{I_{response}({\Delta t})}={\frac{w_i}{\tau_s} }e^{-{\Delta t}/{\tau_s} }
$$

> Response currents from all synapses integrate through the dendrite of the post-synaptic neuron, accumulating potential imbalance in the membrane of the cell, until **reaching a threshold** at which point a post-synaptic action potential is released (which resets the membrane potential)

Firing-rate model is a statistical model of biological neural networks, which assumes that functionality of a biological neural network mostly depends on **frequency patterns of the action potentials** flowing in the network, whose precise timing patterns do not matter.

The firing rate of a neuron is a **function of time $t$** which gives the **frequency density of action potentials** that the neuron would probabilistically release at each time point $t$.

$$
{x(t)}=\lim_{ {\Delta t}\rightarrow 0}\mathbf{E}[\frac{ {spikes{\ }in}[t-{\Delta t},t]}{\Delta t}]
$$

Firing-rate $x_i(\cdot)$ of pre-synaptic neuron $i$ induces a response current $I_i(\cdot)$ at post synaptic neuron. Response currents from all synapses integrate into a total synaptic current $I(\cdot)$ which induces the firing-rate $y(\cdot)$ of the post-synaptic neuron.

Step by step,

1. Response current induced by a single action potential: ${I_{response}(\Delta t)}={\frac{w_i}{\tau_s} }e^{-{\Delta t}/{\tau_s} }$
2. Response current induced by all spikes of synapse $i$: ${I_i(t)}={\int_{-\infty}^{t} }{I_{response}(t-\tau){s_i(\tau)d\tau} }$
3. Total synaptic current induced by all spikes of all synapses: $I(t)=\sum_i{T_i(t)}$

Finally, firing rates $\mathbf{x}$ of all the pre-synaptic neurons collectively induce total synaptic current $I$ at the post-synaptic neuron:

$$
{\tau_s}{\frac{\mathrm{d}I}{\mathrm{d}t} }=-I\,+\,\mathbf{w}\cdot\mathbf{x}
$$

Total synaptic current $I$ determines the firing rate $y$ of the post-synaptic neuron

$$
{\tau_r}{\frac{\mathrm{d}y}{\mathrm{d}t} }=-y+\phi(I)
$$

where $\phi(\cdot)$ is called the **activation function** of the (post-synaptic) neuron.

### Activity-independent Synaptic Plasticity and Hebbian Rule

**Plasticity** is the ability of a solid material to undergo permanent deformation, a persistent change of shape in response to applied forces.

Changes of a synapseâ€™s strength $w$ that persist for tens of minutes or longer are generally called long-term potentiation (LTP) and long-term depression (LTD). Activity-dependent long-term synaptic plasticity is widely believed to be the biological foundation underlying the phenomenon of learning and memory.

The **Hebbian theory** is the best-known model for activity-dependent long-term synaptic plasticity. The **Hebbian rule** states that synapses change in proportion to the covariance between activities of the pre- and post-synaptic neurons.

èµ«å¸ƒç†è®ºï¼ˆè‹±èªï¼šHebbian theoryï¼‰æ˜¯ä¸€ä¸ªç¥ç»ç§‘å­¦ç†è®ºï¼Œè§£é‡Šäº†åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­è„‘ä¸­çš„ç¥ç»å…ƒæ‰€å‘ç”Ÿçš„å˜åŒ–ã€‚èµ«å¸ƒç†è®ºæè¿°äº†çªè§¦å¯å¡‘æ€§çš„åŸºæœ¬åŸç†ï¼Œå³çªè§¦å‰ç¥ç»å…ƒå‘çªè§¦åç¥ç»å…ƒçš„æŒç»­é‡å¤çš„åˆºæ¿€ï¼Œå¯ä»¥å¯¼è‡´çªè§¦ä¼ é€’æ•ˆèƒ½çš„å¢åŠ ã€‚è¿™ä¸€ç†è®ºç”±å”çº³å¾·Â·èµ«å¸ƒäº1949å¹´æå‡ºï¼Œåˆè¢«ç§°ä¸ºèµ«å¸ƒå®šå¾‹ï¼ˆHebb's ruleï¼‰ã€èµ«å¸ƒå‡è¯´ï¼ˆHebb's postulateï¼‰ã€ç»†èƒç»“é›†ç†è®ºï¼ˆcell assembly theoryï¼‰ç­‰[^3]ã€‚

â€œå½“ç¥ç»å…ƒ A çš„ä¸€ä¸ªè½´çªå’Œç¥ç»å…ƒ B å¾ˆè¿‘ï¼Œè¶³ä»¥ å¯¹å®ƒäº§ç”Ÿå½±å“ï¼Œå¹¶ä¸”æŒç»­åœ°ã€é‡å¤åœ°å‚ä¸äº†å¯¹ç¥ç»å…ƒ B çš„å…´å¥‹ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸¤ä¸ªç¥ ç»å…ƒæˆ–å…¶ä¸­ä¹‹ä¸€ä¼šå‘ç”ŸæŸç§ç”Ÿé•¿è¿‡ç¨‹æˆ–æ–°é™ˆä»£è°¢å˜åŒ–ï¼Œä»¥è‡´ç¥ç»å…ƒ A ä½œä¸ºèƒ½ä½¿ ç¥ç»å…ƒBå…´å¥‹çš„ç»†èƒä¹‹ä¸€ï¼Œå®ƒçš„æ•ˆèƒ½åŠ å¼ºäº†ï¼â€è¿™ä¸ªæœºåˆ¶ç§°ä¸ºèµ«å¸ƒç†è®ºï¼ˆHebbian Theoryï¼‰æˆ–èµ«å¸ƒè§„åˆ™ï¼ˆHebbian Ruleï¼Œæˆ– Hebbâ€™s Ruleï¼‰ï¼å¦‚æœä¸¤ä¸ªç¥ç»å…ƒæ€»æ˜¯ç›¸ å…³è”åœ°å—åˆ°åˆºæ¿€ï¼Œå®ƒä»¬ä¹‹é—´çš„çªè§¦å¼ºåº¦å¢åŠ ï¼è¿™æ ·çš„å­¦ä¹ æ–¹æ³•è¢«ç§°ä¸ºèµ«å¸ƒå‹å­¦ä¹  ï¼ˆHebbian learningï¼‰ï¼Hebbè®¤ä¸ºäººè„‘æœ‰ä¸¤ç§è®°å¿†ï¼šé•¿æœŸè®°å¿†å’ŒçŸ­æœŸè®°å¿†ï¼çŸ­æœŸè®° å¿†æŒç»­æ—¶é—´ä¸è¶…è¿‡ä¸€åˆ†é’Ÿï¼å¦‚æœä¸€ä¸ªç»éªŒé‡å¤è¶³å¤Ÿçš„æ¬¡æ•°ï¼Œæ­¤ç»éªŒå°±å¯å‚¨å­˜åœ¨é•¿ æœŸè®°å¿†ä¸­ï¼çŸ­æœŸè®°å¿†è½¬åŒ–ä¸ºé•¿æœŸè®°å¿†çš„è¿‡ç¨‹å°±ç§°ä¸ºå‡å›ºä½œç”¨ï¼äººè„‘ä¸­çš„æµ·é©¬åŒºä¸º å¤§è„‘ç»“æ„å‡å›ºä½œç”¨çš„æ ¸å¿ƒåŒºåŸŸ[^4]

- Let real number $w$ denote the **strength of an excitatory synapse**, $x(t)$ and $y(t)$ the pre- and post-synaptic firing rates at time $t$,

$$
{\tau_w}{\frac{\mathrm{d}w}{\mathrm{d}t} }=x(t){\,}y(t)
$$

- Let **random variable** $X$ and $Y$ be the pre- and post-synaptic firing rates at a random time $\tau$ in a time window $[t-{\Delta t},t]$,

$$
{\tau_w}{\frac{\mathrm{d}w}{\mathrm{d}t} }=\mathbf{E}[XY]-{\mathbf{E}[X]}{\mathbf{E}[Y]}
$$

## Artificial Neural Networks, ANN

> ANNåœ¨ç»Ÿè®¡é¢†åŸŸç”±äºéš¾ä»¥è¯æ˜ï¼Œè¿‘å‡ å¹´è™½ç„¶åœ¨åŠ¨åŠ›ç³»ç»Ÿã€æŠ½è±¡ä»£æ•°çš„æ–¹å‘ä¸Šæœ‰ç ”ç©¶ï¼Œä½†å¯è§£é‡Šæ€§è¿˜æ˜¯æ¯”è¾ƒå·®ï¼›ç”Ÿç‰©ç§‘å­¦æ–¹é¢ï¼Œè¿‡äºå¤æ‚çš„è„‘ç¥ç»ç³»ç»Ÿä»ç„¶éš¾ä»¥æ¨¡ä»¿ï¼Œä¸è¿‡è¯è¯´å›æ¥ï¼Œå…¨è„‘æ¨¡æ‹Ÿä¹Ÿç¡®å®æ˜¯ä¸€ä¸ªç ”ç©¶æ–¹å‘ï¼›æœ€åï¼ŒAIé¢†åŸŸæ›¾åœ¨æ—©æœŸè¢«æå‡ºï¼Œä½†ç”±äºè¿‡äºåºå¤§è¢«æç½®äº†æ•°åå¹´

Parameterization in AI:

- Neural-network models
- Tabular models
- Non-differentiable models

### Multi-Layer Perceptron, MLP

![image-20220707142039567](https://tva1.sinaimg.cn/large/e6c9d24ely1h3ycb4jbvej20si0jcn08.jpg)

Each hidden-layer and ouput-layer node $i$ repoesents a sub-mode $g(\mathbf{x};\mathbf{w_i},b_i)=\phi({\mathbf{w_i}\cdot{\mathbf{x} }+{b_i} })$. If define $\mathbf{\bar{x} }=(\mathbf{x},1)^T,{\;}\bar{\mathbf{w_i} }=(\mathbf{w_i},{b_i})$

### Activitation Functions

- Sigmoid, it can seperate when the **input is too large**:
$$
\sigma(x)=\frac{1}{1+e^{-x} }
$$

- tanh: $\tanh(x)$
- ReLU: $max(0,x)$
- Leaky
- ReLU
- Maxout
- ELU

ä¸€ä¸ªMLPæ¨¡å‹çš„éœ€è¦çš„å‚æ•°æ˜¯éå¸¸å¤šçš„ï¼Œä»¥ä¸€ä¸ªè¾“å…¥$\mathbb{R}^5$ï¼Œæœ‰ä¸¤ä¸ª$\mathbb{R}^7$éšè—å±‚å¹¶è¾“å‡º$\mathbb{R}^4$çš„ç½‘ç»œæ¥è®²ï¼Œä»–éœ€è¦çš„å‚æ•°æ˜¯

$$
n=[(N_{input}+1)\cdot{N_{next} }]+[(N_{hidden1}+1)\cdot{N_{next} }]+[(N_{hidden2}+1)\cdot{N_{output} }]=170
$$

A **decision policy** determines the probability that an â€œintelligentâ€ system would output a particular â€œactionâ€ in a time step conditioned on aparticular â€œstateâ€ of the system.

A policy function ğœ‹ is a conditional probability distribution. As a probability distribution, any policy function has to satisfy the following constraints

![image-20220707144028629](https://tva1.sinaimg.cn/large/e6c9d24ely1h3ycvp1fuaj20wu0fuad9.jpg)

For a discrete output, define **softmax function** and add it to the output layer

$$
\sigma:\mathbb{R}^K{\rightarrow}[0,1]^K,{\;}where{\;}\sigma_i(z)={\frac{e^{z_i} }{\sum_{j=1}^K}{e^{z_j} }},{\,} for{\,} i=1,\dotsb,K
$$

> ç”±äºæŒ‡æ•°çš„å¢é•¿é€Ÿç‡å¾ˆå¤§ï¼Œè¿™ä¸ªå‡½æ•°å¯ä»¥â€œæŸ”è½¯â€çš„æ”¾å¤§è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œå¹¶å…±åŒæˆä¸ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒå‡½æ•°ã€‚</br>
> Softmax functionæ˜¯ä¸ºäº†å°†è¾“å‡ºè½¬æ¢ä¸ºä¸€ä¸ªæ¡ä»¶æ¦‚ç‡ï¼Œè€Œä¸æ˜¯è·å¾—ä¸€ä¸ªæœ€å¤§å€¼ã€‚

Then $\pi(\mathbf{s},\mathbf{a};\mathbf{\theta})=\sigma(f_{MLP}(\mathbf{s};\mathbf{\theta}))\cdot\mathbf{a}$

å…¶ä¸­$\mathbf{a}$æ˜¯ä¸€ç»„one-hot vectorã€‚

![image-20220707144606874](https://tva1.sinaimg.cn/large/e6c9d24ely1h3yd81y6lyj21400fgq6i.jpg)

### Example: Locomotion Control

### Domain-specific NN architecture

#### Convolutional Network

![image-20220707153056204](https://tva1.sinaimg.cn/large/e6c9d24ely1h3yec7ew8rj214u0q6tc2.jpg)

#### Transformer Network

![image-20220707154021201](https://tva1.sinaimg.cn/large/e6c9d24ely1h3yem057wej21z60magpd.jpg)

#### Graph Network

![image-20220707154326848](https://tva1.sinaimg.cn/large/e6c9d24ely1h3yep7une0j210g0jmwfq.jpg)

## Tabular Model and Non-differential Model

A tabular model is a parametric model that covers the entire policy space.

## Biological Evolution

## HomeWorkds

### homework-1.a

## References

[^1]: [æœºå™¨å­¦ä¹ â€”â€”çº¿æ€§é«˜æ–¯æ¨¡å‹](https://www.cnblogs.com/young978/p/15813842.html)
[^2]: [é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰ - æˆ´æ–‡äº®çš„æ–‡ç«  - çŸ¥ä¹](https://zhuanlan.zhihu.com/p/30483076)
[^3]: [èµ«å¸ƒç†è®º - ç»´åŸºç™¾ç§‘](https://zh.m.wikipedia.org/zh/èµ«å¸ƒç†è®º)
[^4]: [æœºå™¨å­¦ä¹ ç®€ä»‹](https://blog.csdn.net/qq_40808154/article/details/115407544)
